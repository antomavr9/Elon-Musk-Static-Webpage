import numpy as np
from sklearn.linear_model import LinearRegression
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LogisticRegression

# Generate some sample data
X = np.random.randn(100, 5)
y = np.random.randn(100)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the two models on the training set
model1 = LinearRegression().fit(X_train, y_train)
model2 = LGBMRegressor().fit(X_train, y_train)

# Generate predictions for the testing set
preds1 = model1.predict(X_test)
preds2 = model2.predict(X_test)

# Combine the predictions of the two models into a single feature vector
ensemble_input = np.concatenate((preds1.reshape(-1, 1), preds2.reshape(-1, 1)), axis=1)

# Train a meta-classifier on the combined predictions
meta_classifier = LogisticRegression().fit(ensemble_input, y_test)

# Evaluate the performance of the ensemble on the testing set
ensemble_preds = meta_classifier.predict(ensemble_input)
ensemble_rmse = mean_squared_error(y_test, ensemble_preds, squared=False)
print("Ensemble RMSE:", ensemble_rmse)
